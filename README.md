# ğŸš€ Vision Pipeline Compiler

Turn a **natural language prompt** into a **runnable AI vision pipeline**.

This project converts user prompts like:

- "Detect persons and count them"
- "Segment people in an image"
- "Classify this image"
- "Pose estimation of humans"

into a **ready-to-run Python project** using **YOLOv8 pre-trained models**.

---

## ğŸ§  How It Works (High Level)

```bash
User Prompt
â†“
LLM â†’ Structured JSON
â†“
Schema Validation
â†“
Execution Plan (Compiler)
â†“
Template-Based Code Generation
â†“
Runnable AI Vision Project (ZIP)

```

---
## â„¹ï¸ Important Information

This project uses **YOLOv8 pre-trained models** provided by Ultralytics.  
No model training or fine-tuning is performed.

Based on the userâ€™s prompt, the system:
- Selects the appropriate YOLO model and task
- Generates a complete runnable pipeline using templates
- Packages the generated pipeline into a **ZIP file**

### ğŸ“¦ Generated ZIP File

The downloaded ZIP file contains:
- `run.py` â€“ executable inference script  
- `requirements.txt` â€“ all required dependencies  
- `README.md` â€“ instructions to run the pipeline  

You can extract the ZIP file and run the pipeline independently on any machine.

### â–¶ï¸ Running the Generated Pipeline

All instructions are already included inside the generated ZIP.  
In general, the steps are:

```bash
pip install -r requirements.txt
python run.py image.jpg
```
---
## ğŸ§© Architecture Overview

### 1ï¸âƒ£ Base Template
- Loads model
- Loads input image
- Runs inference **once**
- Provides hooks for extensions

### 2ï¸âƒ£ Task Templates (Capabilities)
- Detection
- Segmentation
- Classification
- Pose Estimation

These **only interpret model results**, they never run inference again.

### 3ï¸âƒ£ Output Templates
- Bounding boxes
- Masks
- Count
- Count per class
- Classification output

These **consume prepared variables** and produce outputs.

---

## ğŸ“ Project Structure

```bash
backend/
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ prompt_to_json.py
â”‚   â”œâ”€â”€ validator.py
â”‚
â”œâ”€â”€ compiler/
â”‚   â”œâ”€â”€ pipeline_builder.py
â”‚   â”œâ”€â”€ code_generator.py
â”‚   â”œâ”€â”€ task_router.py
â”‚   â”œâ”€â”€ output_resolver.py
â”‚   â”œâ”€â”€ model_resolver.py
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ base.py.jinja
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”œâ”€â”€ detect.py.jinja
â”‚   â”‚   â”œâ”€â”€ segment.py.jinja
â”‚   â”‚   â”œâ”€â”€ classify.py.jinja
â”‚   â”‚   â””â”€â”€ pose.py.jinja
â”‚   â”œâ”€â”€ outputs/
â”‚       â”œâ”€â”€ detect.py.jinja
â”‚       â”œâ”€â”€ count.py.jinja
â”‚       â”œâ”€â”€ masks.py.jinja
â”‚       â””â”€â”€ classify.py.jinja
â”‚
â”œâ”€â”€ runtime/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ api/
    â””â”€â”€ main.py
    â””â”€â”€ routes.py
    â””â”€â”€ errors.py
    â””â”€â”€ schemas.py

backend/
â”œâ”€â”€ app.py

```
---

## ğŸ˜ğŸ¤¯ğŸ¥³Output Screenshots
<img width="1920" height="1020" alt="image" src="https://github.com/user-attachments/assets/3d25442d-439d-415b-b850-ddf7b17ecf1d" />
<img width="1920" height="1020" alt="image" src="https://github.com/user-attachments/assets/783eda0c-d933-4d81-988e-fd2460838a88" />
<img width="1920" height="1020" alt="image" src="https://github.com/user-attachments/assets/3149f659-e297-48b6-919d-dd7c0391dceb" />

## ğŸš€ Get Started

Follow the steps below to run the Vision Pipeline Compiler locally.

---

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/vision-pipeline-compiler.git
cd Prompt-to-Simple-Vision-Model
```
### 2ï¸âƒ£ Create and Activate a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```
### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r runtime/requirements.txt
```

### 4ï¸âƒ£ Move to backend folder

```bash
cd backend
```

### 5ï¸âƒ£ Configure Environment Variables
Past your API key in .env file inside the backend/ directory:
```bash
GROQ_API_KEY=your_api_key_here
```

## â–¶ï¸ Running Backend and Frontend Together

The backend and frontend run as **two separate services**.  
Open **two terminals** and start each one as shown below.


### ğŸ–¥ï¸ Terminal 1 â€” Backend (API)

```bash
uvicorn api.main:app --reload
```

Example

```bash
(venv) PS C:\Users\dhanu\Downloads\Prompt-to-Simple-Vision-Model\backend> uvicorn api.main:app --reload
INFO:     Will watch for changes in these directories: ['C:\\Users\\dhanu\\Downloads\\Prompt-to-Simple-Vision-Model\\backend']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [28300] using StatReload
INFO:     Started server process [20088]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

### ğŸ–¥ï¸ Terminal 2 â€” Frontend

```bash
python -m venv venv
cd frontend
python app.py
```
Example

```bash
PS C:\Users\dhanu\Downloads\Prompt-to-Simple-Vision-Model> venv\Scripts\activate
(venv) PS C:\Users\dhanu\Downloads\Prompt-to-Simple-Vision-Model> cd frontend
>> python app.py
* Running on local URL:  http://127.0.0.1:7860
* Running on public URL: https://8aea232bd1ea5a1e43.gradio.live

This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)
```
Click the IP address generated to run the model locally in your browser or the link to use in some other devices.
